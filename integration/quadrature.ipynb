{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we begin our study of the numerical integration of functions.  We may encounter cases where we want to obtain a numerical approximation of the integral of some function *f(x)* on an interval $a \\le x \\le b$, $I(f) = \\int_a^b f(x) dx$.  One class of numerical techniques approximates $I(f)$ as a sum of a series of a finite number of terms, each term proportional to the value of $f(x)$ somewhere on the interval, i.e. $I(f) \\approx \\sum_{i=0}^{i=N} \\alpha_i f(x_i)$.  The $\\alpha_i$ are finite coefficients.  One way to determine the coefficients approximates $f(x)$ with an interpolating function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want the integral of a function $f(x)$ over the interval $a\\le x\\le b$.  The simplest choice of an interpolating function assumes that $f(x)$ is constant on the interval, i.e. use an interpolating function $C(x) = f(a)$.  Then, the integral is approximated by the area of the rectangle subtended by $C(x), I(f) \\approx (b-a)f(a)$.  A better choice for a constant interpolant is the value of $f(x)$ at the midpoint, i.e. $C(x) = f\\left(\\frac{b+a}{2}\\right)$.  Then, $I(f) \\approx (b-a)f\\left(\\frac{b+a}{2}\\right)$.  The *midpoint rule* is exact for linear functions $f(x) = \\alpha x + \\beta$.  But it's a poor approximation for higher order functions.  For example, for $f(x) = x^2, \\int_0^1 f(x) dx = \\frac{1}{3}$ while the midpoint approximation is $I(f) \\approx (1-0)f\\left(\\frac{1}{2}\\right) = \\frac{1}{4}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve on the midpoint rule by using a linear interpolant $C(x) = f(a) + \\frac{f(b)-f(a)}{b-a}(x-a)$. The integral is then approximated by the volume of the trapezoid subtended by $C(x), I \\approx (b-a)f(a) + \\frac{1}{2}(b-a)*[f(b)-f(a)] = \\frac{b-a}{2}\\left[f(b)+f(a)\\right]$.  This is known as the *trapezoid rule*.  We can improve more if we use a quadratic interpolating function, choosing $C(x)$ so that it passes through the points $(a,f(a)), (\\frac{a+b}{2},f(\\frac{a+b}{2})), (b,f(b))$.  The integral is then approximated by the area subtended by the parabola $C(x)=\\alpha x^2+\\beta x + \\gamma$.  \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f(b) &=& \\alpha b^2 + \\beta b + \\gamma \\\\\n",
    "f(a) &=& \\alpha a^2 + \\beta a + \\gamma \\\\\n",
    "f(\\frac{a+b}{2}) &=& \\alpha \\left(\\frac{a+b}{2}\\right)^2 + \\beta \\left(\\frac{a+b}{2}\\right) + \\gamma \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Evaluating (3) - [(1)+(2)]/2 and solving for $\\alpha$,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha = \\frac{4}{(b-a)^2}\\left[\\frac{f(b)+f(a)}{2}-f\\left(\\frac{b+a}{2}\\right)\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Similarly, evaluating (1)-(2) and solving for $\\beta$,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\beta = \\frac{1}{b-a}\\left[f(b)-f(a)-\\alpha(b^2-a^2)\\right].\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Evaluating (1)+(2) and substituting for $\\alpha,\\beta$,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\gamma = \\frac{f(b)+f(a)}{2} - \\frac{\\alpha}{2}\\left(b^2+a^2\\right) - \\frac{\\beta}{2}(b+a)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Evaluating the integral, \n",
    "\n",
    "$$\n",
    "\\int_a^b C(x) dx = \\int_a^b (\\alpha x^2 + \\beta x + \\gamma) dx = \\frac{\\alpha}{3}\\left(b^3-a^3\\right) + \\frac{\\beta}{2}\\left(b^2-a^2\\right)+\\gamma\\left(b-a\\right)\n",
    "$$.\n",
    "\n",
    "Summing terms in $\\beta$ and $\\gamma$,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\beta}{2}(b^2-a^2)+\\gamma(b-a) &=& \\frac{\\beta}{2}(b^2-a^2)+(b-a)\\frac{f(b)+f(a)}{2} - \\frac{\\alpha}{2}(b^2+a^2)(b-a) - \\frac{\\beta}{2}(b^2-a^2) \\\\ \n",
    "&=& (b-a)\\frac{f(b)+f(a)}{2} - \\frac{\\alpha}{2}\\left(b^2+a^2\\right)(b-a) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Substituting and simplifying,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\alpha}{3}\\left(b^3-a^3\\right) + \\frac{\\beta}{2}\\left(b^2-a^2\\right)+\\gamma\\left(b-a\\right) &=& \\frac{\\alpha}{3}\\left(b^3-a^3\\right) + (b-a)\\frac{f(b)+f(a)}{2} - \\frac{\\alpha}{2}\\left(b^2+a^2\\right)(b-a) \\\\\n",
    "&=& -\\frac{\\alpha}{6}(b-a)^3+(b-a)\\frac{f(b)+f(a)}{2} \\\\\n",
    "&=& -\\frac{2(b-a)}{3}\\left[\\frac{f(b)+f(a)}{2}-f\\left(\\frac{b+a}{2}\\right)\\right]+(b-a) \\frac{f(b)+f(a)}{2} \\\\\n",
    "&=& \\frac{b-a}{3}\\frac{f(b)+f(a)}{2}+\\frac{2(b-a)}{3}f\\left(\\frac{b+a}{2}\\right) \\\\\n",
    "&=& \\frac{b-a}{6}\\left[f(a)+4f\\left(\\frac{b+a}{2}\\right)+f(b)\\right]\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we could continue to derive higher order quadrature rules using interpolating polynomials of arbitrary degree.  This approach, known as *Newton-Coates*, suffers though from undesirable mathematical and computational behavior.  Improved estimates can be obtained with composite rules, representing the integral as a finite sum and approximating the $f(x)$ on each sub-interval with an interpolating polynomial.  Thus, the quadrature rules considered above can be expressed as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "I(f) &\\approx& \\sum_{i=1}^{N} w_i f\\left(\\frac{x_{i-1}+x_i}{2}\\right) \\\\\n",
    "I(f) &\\approx& \\sum_{i=1}^{N} \\frac{w_i}{2}\\left[f(x_{i-1})+f(x_i)\\right] \\\\\n",
    "I(f) &\\approx& \\sum_{i=1}^{N} \\frac{w_i}{6}\\left[f(x_{i-1})+4f\\left(\\frac{x_{i-1}+x_i}{2}\\right)+f(x_i)\\right] \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "for the midpoint, trapezoid, and Simpson's rules, respectively.  In these expressions, $w_i$ is the width of the interval $(x_{i-1},x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write functions that implement the composite midpoint, trapezoid, and Simpson's rules.  Test your implementations for some simple functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a more complex function.  **Use the methods your wrote above to evaluate the error function**, $\\textrm{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_0^z e^{-t^2} dt$.  **Plot the error function evaluated with the three quadrature rules and with the erf function available from scipy.  What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to have a measure that allows us to compare and quantify the performance of the different approximations.  One simple metric is the absolute error, $\\epsilon_{\\textrm{abs}} = |I-I_h|$, where $I_h$ is the approximated value of the integral assuming uniform subintervals of width $h$.  **Calculate the absolute error for the error function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior observed is described by the *polynomial interpolation theorem*.  It can be understood considering the function as a Taylor series expansion, which we state here without proof:\n",
    "\n",
    "Let $f(x)$ be a function with $n+1$ continuous derivatives on an interval containing $n+1$ distinct points $x_0 < x_1 < \\ldots < x_n$.  If $p(x)$ is the unique polynomial of degree $n$ or less such that $p(x_i) = f(x_i)$, then for any value of $x$ in the interval $[x_0,x_n]$,\n",
    "\n",
    "$$\n",
    "f(x) - p(x) = \\frac{\\left(x-x_0\\right)\\left(x-x_1\\right)\\cdots\\left(x_n\\right)}{\\left(n+1\\right)!}f^{\\left(n+1\\right)}(x),\n",
    "$$\n",
    "\n",
    "where $f^{\\left(n+1\\right)}$ is the $(n+1)^{\\textrm{th}}$ derivative of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the integral $I = \\int_{-1}^{1} e^{x} dx = e - (1/e)$.  This integrand is a polynomial of infinite order.  Do you expect the methods above to approximate well this integral (and why)? Let's plot the function, evaluate the integral for 5 subintervals, and calculate the absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the above theorem for the trapezoid rule. Here, the interpolating polynomial on each subinterval is a linear function, so the error in this approximation in the $i^{\\textrm{th}}$ subinterval is given by\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "|I-I_h|_i &=& |\\int_{x_{i-1}}^{x_i} \\frac{\\left(x-x_{i-1}\\right)\\left(x-x_i\\right)}{2}f^{(2)}(z) dx| \\\\\n",
    "&=& \\frac{\\left(x_i-x_{i-1}\\right)^3}{12}f^{(2)}(z) \\\\\n",
    "&=& \\frac{h^3}{12}f^{(2)}(z).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Summing the error in each of the $n$ (uniform) subintervals, substituting for $f^{(2)}(z)$ the maximum value of $f^{(2)}$ on $[x_0,x_n]$, and using the relation $nh = (x_n-x_0)$, we obtain\n",
    "\n",
    "$$\n",
    "|I-I_h| \\le \\frac{M_2}{12}(x_n-x_0)h^2.\n",
    "$$\n",
    "\n",
    "Can you find an expression for the bound on the error in the midpoint and Simpson's method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods considered appear to approximate the integral with only a few subintervals, at least for the functions we've considered so far.  How qiuckly a numerical algorithm converges to the true answer, a concept called *convergence*, is critical in numerical analysis. To attempt to quantify this concept, we need a measure how closely the approximation is to the true value.  One measure we can use is the absolute error defined above.  Let's evaluate $I = \\int_{-1}^{1} e^{x} dx$ using the three methods for different number (or size) subintervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the error decreases with decreasing $h$.  On a log-log plot, the slope is 2 for the midpoint and trapezoid methods, suggesting that the error in the midpoint method is also quadratic in the width of the sub-interval.  What is the slope of the line corresponding to the error in Simpson's method?  You should try to reconcile the numerical results above with the theoretical error bound obtained applying the polynomial interpolation theorem for the different quadratures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we may want to conclude that we can obtain an arbitrarily good approximation of the integral $I$ by continuing to decrease $h$.  However, our discussion above only addressed the *discretization* or *approximation* error resulting from the numerical approximation, i.e. from the approximation of $f(x)$ by an interpolating function on each subinterval.  There is another type of error we must remain cognizant of.  *Roundoff error* arises due to the finite precision of the representation of numbers in binary by the computer.  The importance of roundoff error depends on the precision with which we represent numbers.  Thus, for example, single precision floating point numbers are uncertain at the level of $10^{-8}$.  Similarly, double and quadruple precision numbers are accurate to $10^{-16}$ and $10^{-32}$, respectively.  \n",
    "\n",
    "The approximation error goes to 0 as $h\\rightarrow0$.  However, as $h$ decreases, the number of function evaluations and sums in the terms increases inversely.  Let's explore this by repeating the exercise above for a broader range of values of $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
